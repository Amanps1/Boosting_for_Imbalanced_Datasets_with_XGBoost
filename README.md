
# Boosting for Imbalanced Datasets with XGBoost

## Objective
To improve classification performance on imbalanced datasets using XGBoost and imbalance-handling techniques.

## Methodology
- Exploratory Data Analysis (EDA)
- Data preprocessing
- Handling class imbalance using SMOTE and scale_pos_weight
- Training XGBoost classifier
- Model evaluation using appropriate metrics

## Technologies Used
- Python
- XGBoost
- Scikit-learn
- Pandas
- NumPy
- Matplotlib
- Imbalanced-learn

## Evaluation Metrics
- Precision
- Recall
- F1-score
- ROC-AUC
- Confusion Matrix

## Conclusion
XGBoost combined with imbalance-aware techniques significantly improves minority class prediction.
